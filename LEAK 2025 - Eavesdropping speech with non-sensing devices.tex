\documentclass[sigconf, nonacm]{acmart}

\usepackage{tabularx}
\usepackage{makecell}
\usepackage{color,soul}

\newcolumntype{C}{>{\centering\arraybackslash}X} 

\AtBeginDocument{
  \providecommand\BibTeX{{
    Bib\TeX}}}

\acmConference[LEAK '25]{Unusual Side Channels and Privacy Leaks}{Februrary 07, 2025}{Berlin, Germany}
\setcopyright{cc}
\setcctype[4.0]{by-sa}
\acmDOI{}
\acmISBN{}

\begin{document}

\title{Eavesdropping Speech with Non-sensing Devices}

\author{Tim Holzhey}
\affiliation{%
  \institution{Technische Universit√§t Berlin}
  \city{Berlin}
  \country{Germany}}
\email{holzhey@campus.tu-berlin.de}

\begin{abstract}
  % Topic + Relevance
  In recent years, numerous research papers have shown that air pressure waves produced by human speech or other sounds can induce vibrations into an array of non-acoustic sensors (e.g. motions sensors) or into externally measured objects (e.g. laser-based vibrometer) skewing sensor readings in a reversable manner, effectively turning them into undisclosed microphones.
  This allows for eavesdropping on private speech by maliciously altered devices and therefore posing a real threat to privacy when exploited.
  
  % Objective of this paper
  This work will examine and compare different types of vibration-based side channel attacks employed on common IoT and Smart devices to recover speech or infer privacy-sensitive information about the speaker like their identity, political views or gender.
  We explore the steps taken to take control of the targeted device, gather the necessary data, and perform signal processing and machine learning techniques to extract audible information from the sensor readings.
  The overview established over the attacks then allows for a comprehensive feasability study for the respective attack methods and complexity required to perform such attacks in a real world scenario. 
  We discuss possible countermeasures to mitigate the risk of such attacks and provide an outlook on future research directions in the field.
\end{abstract}

\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10002978.10003001.10010777.10011702</concept_id>
  <concept_desc>Security and privacy~Side-channel analysis and countermeasures</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10002978.10003001.10003003</concept_id>
  <concept_desc>Security and privacy~Embedded systems security</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10010520.10010553.10010559</concept_id>
  <concept_desc>Computer systems organization~Sensors and actuators</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Side-channel analysis and countermeasures}
\ccsdesc[500]{Security and privacy~Embedded systems security}
\ccsdesc[500]{Computer systems organization~Sensors and actuators}

\keywords{Security, Privacy, Side-channel, Eavesdropping, Speech, Acoustic, Hardware Security, Privacy Leaks}

\received{7 January 2025}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\maketitle

\section{Introduction}
While the IoT market is on the rise and still growing exponentially, projected to exceed USD 4 trillion by 2032 \cite{FortuneBusinessInsights2024}, this opens up a new attack vector for adversaries to exploit in addition to traditional software vulnerabilities in computers.
Latest surveys show that the American households had on average 21 connected devices \cite{DeloitteConnectedConsumer2023}, a relevant part of which are IoT and Smart Home devices.
IoT devices are often equipped with a variety of sensors to interface with their physical environment, such as accelerometers, gyroscopes, microphones, and cameras.
Many of these sensors can also be found in modern smartphones, which are carried around by most people\footnote{Surveys from 2024 suggest that 91 \% of Americans own a smartphone \cite{PewResearchMobileFactSheet2024}}. Mobile operating systems provide zero-permission access to sensor data from the built-in accelerometer and gyroscope, therefore have been the subject of the majority of research done in this field.
The findings from vulnerabilities found in smartphones can be projected onto IoT and Smart devices with similar sensors that do not have a primary function of audio recording i.e. do not have a built-in microphone ("non-sensing").
To execute a vibration-based eavesdropping attack, most of the previous papers took the approuch to exploited MEMS\footnote{Abbr. Micro-electromechanical systems} motion sensors (accelerometers, gyroscopes and magnetometers) commonly found in smartphones and many smart devices including smartwatches, fitness trackers, gaming controllers, etc.
Some of the more experimental approaches have also shown that other sensors like Lidar scanners in vacuum cleaners, the position error signal of write heads in hard drives or electro-optical sensors directed at ceiling lights can be exploited for similar attacks.

\textbf{CONTRIBUTION:} Altough parts of the available research material in this field is investigating keystroke recovery attacks \cite{Touchtone2023}\cite{TouchLogger2011}\cite{TapLogger2012} or is using sophisticated external setups (e.g. RFID-Tags \cite{RFMic2023}, millimeter-waves \cite{MMMic2022}, WiFi radio \cite{ART2015}), we limit the scope of this paper to \textbf{on-device vibration-based speech and general sound recovery attacks}.
This includes attacks in theory possible without any modified or additional hardware assuming a compromised device or malicious software.
This work aims to provide a comprehensive overview of the current state of research in the field of vibration-based eavesdropping attacks on non-sensing devices.
We highlight notable research papers and their findings, compare the different attack methods and achieved results, and discuss the feasibility of such attacks in real-world scenarios.

\section{Background and Literature Review}
\subsection{Vibration-based Eavesdropping Attacks}
Sound created by a human speaking or any other sound can be characterized as spatially and temporally propagating changes in air pressure in the audible frequency range (20 Hz - 20 kHz).
Similarly to how sound waves induce vibrations into our eardrums to let us perceive sound, they can also couple vibrations into all other objects they encounter, more so into objects that are resonant at the frequency of the sound.
In a typical microphone, an oscillating diaphragm is used to convert these vibrations into an electrical signal i.e. a change in voltage by varying the capacitance of a capacitor (condenser microphone) or by inducing a current into a coil (dynamic microphone).
Even if unintended, the same phenomenon can be used to turn any other sensing electrical component into a microphone if it has a moving part capable of influencing the electrical properties of the component directly (e.g. MEMS, write head of a hard drive) or observing the movement of another object (e.g. laser vibrometer, Lidar scanner, camera).
As audible information was not intended to be captured by these sensors, an attacker who is able to recover this information from the sensor readings is exploiting a side channel vulnerability.

\subsection{MEMS-based Eavesdropping Attacks}

Sensors manufactured using micro-electromechanical fabrication techniques (MEMS) incorporate electronics and moving parts on a micrometer-scale chip to measure physical parameters like acceleration (accelerometer), orientation and angular velocity (gyroscope) or the magnetic field (magnetometer).
The manufactureing process makes use of lithography and etching semiconductor manufacturing techniques on silicon wafers that allows for the production of small, low-cost sensors with high sensitivity and accuracy.
They are widely used in consumer electronics to enable features like screen rotation, step counting, navigation and gaming feedback.
On a physical level, MEMS sensors are most commonly realized by a spring-suspended proof mass that changes the capacitance of the circuitry when displaced (variable capacitance MEMS) or by a flexible piezoelectric material that changes its electrical resistance when bent (piezoresistive MEMS).
The structures can be repeated and aligned in three orthogonal directions to measure the physical property in the three-dimensional spacial domain.
\\~\\
\textbf{MEMS Accelerometer:} An accelerometer measures the proper acceleration (change in velocity) of an object relative to a local inertial reference frame.
In the gravitational field of the earth, the accelerometer's measurement is offset by the upwards acceleration of 1 g (9.81 m/s\textsuperscript{2}) relative to the free-falling reference frame.
The basic mechanical structure of an accelerometer consists of a damped proof mass suspended by springs that is displaced when the sensor is accelerated in the opposite direction of movement.
In a typical VC MEMS accelerometer, the proof mass moves between air-gapped fixed electrodes forming a variable capacitor as shown in Figure \ref{fig:MEMS_accelerometer}.
\\[6pt]
\textbf{MEMS Gyroscope:} A gyroscope measures the angular velocity (rate of rotation) of an object relative to a local inertial reference frame.
Gyroscopes realized as a MEMS sensor are commonly Vibrating structure gyroscopes (VSG) that measure the Coriolis force acting on a vibrating proof mass when the sensor is rotated.
As the vibrating mass tends to continue vibrating in the same plane, the Coriolis force deflects the mass in the direction perpendicular to the rotation axis.
The deflection is measured by capacitive sensing or piezoresistive sensing and is proportional to the angular velocity of the rotation as shown in Figure \ref{fig:MEMS_gyroscope}.
\\[6pt]
\textbf{MEMS Magnetometer:} A magnetometer measures the strength and direction of the local magnetic field.
MEMS-based magnetometers often use the Lorentz force acting on the current-carrying conductor in the magnetic field to move the mechanical structure.
The displacement is then measured by capacitive, piezoresistive or optical sensing and is proportional to the magnetic field strength.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{embed/MEMS_accelerometer.png}
  \caption{Accelerometer VC MEMS structure, \\Source: \textit{AccelEve} 2020 \cite{AccelEve2020}}
  \Description{Schematic of a MEMS accelerometer structure}
  \label{fig:MEMS_accelerometer}

  \vspace{0.25cm}

  \includegraphics[width=\linewidth]{embed/MEMS_gyroscope.png}
  \caption{Gyroscope VC MEMS structure, \\Source: \textit{AccelEve} 2020 \cite{AccelEve2020}}
  \Description{Schematic of a MEMS gyroscope structure}
  \label{fig:MEMS_gyroscope}
\end{figure}

Although MEMS sensors are designed to be best insensitive to acoustic noise which could degrade their signal-to-noise ratio, they are still susceptible to sound waves that induce vibrations in the sensor structure.
A MEMS-based eavesdropping attack exploits this vulnerability by recovering the sound-induced vibrations from the sensor readings and reconstructing the original sound.
\\[6pt]
\textbf{Early Work:} The first and most cited paper able to demonstrate the feasibility of recovering speech from motion sensors was \textit{Gyrophone: Recognizing Speech from Gyroscope Signals} \cite{Gyrophone2014} in 2014, a joint efffort by Yan Michalevsky \textit{et al.} from Stanford University and the National Research \& Simulation Center (Rafael Advanced Defense Systems Ltd., Isreal).
The authors showed that a smartphone's gyroscope can be used to recover speech rendered by a nearby loudspeaker using sensor readings at a well below Nyquist sampling rate of 200 Hz.
An Android app was developed to record the gyroscope readings without requiring any special permissions.
Later, they used the off-the-shelf \textit{Sphinx} speech recognition system to recognize spoken digits, but also trained custom machine learning models in \textit{Matlab} to identify the speaker and their gender.
With a limited dictionary of spoken english digits (0-9), no assumptions about the speaker and a distance of 10 cm between the loudspeaker and the smartphone on a solid table surface, a recognition accuracy of at most 26 \% was achieved.
\\~\\
\textbf{Recent Work:} Notably in 2023, Shijia Zhang \textit{et al.} from The Pennsylvania State University leveraged speech eavesdropping attacks using motion sensors in their paper \textit{I Spy You: Eavesdropping Continuous Speech on Smartphones via Motion Sensors} \cite{ISpyU2023} to, for the first time, provide full continuous speech recognition by jointly using accelerometer and gyroscope data.
With a large dictionary of 9950 words and a custom ASR (Automatic Speech Recognition) deep learning model, they achieved 53.3 \% accuracy in recognizing spoken words on an Android smartphone at a sampling rate of 200-500 Hz.

Most recently in 2024, Qingsong Yao \textit{et al.} from Xidian University, China published the paper \textit{Watch the Rhythm: Breaking Privacy with Accelerometer at the Extremely-Low Sampling Rate of 5Hz} \cite{WatchTheRhythm2024} in which they demonstrated that a smartphone's accelerometer can be used for eavesdropping attacks even when limited to a sampling rate of just 5 Hz.
This was achieved by extending the machine learning algorithms to not only consider time-frequency features (spectral) but also the temporal dynamics of the signal (pause rhythm and energy intensity rhythm).
To benchmark against previous papers, the authors showed that english spoken digits could be recognized with an accuracy of 32.70 \% at 5 Hz with an on-device Android app reading accelerometer data.

\subsection{Laser-based Eavesdropping Attacks}

Laser-based measurement devices (e.g. LiDAR scanners, interferometers, vibrometers) are used in various applications to measure precise distances, velocities and material properties contactlessly in various scientific, industrial and medical applications, but recently also in consumer electronics.
Prominently, LiDAR scanners are making their way into our daily lifes as they are used in autonomous vehicles, drones, robotic vacuum cleaners, Apple FaceID or Augumented Reality enabled smartphones.
\\~\\
\textbf{LiDAR Scanner:} LiDAR (Light Detection and Ranging) is a remote sensing and imaging technique that uses a pulsed laser beam to measure the distance to a target object by measuring the time it takes for the light to reflect back to the sensor (Time of Flight).
3D LiDAR scanners can map the environment in all directions by rotating the laser beam in a horizontal plane and measuring the distance at different angles (Figure \ref{fig:LiDAR}).
The cummulative distance measurements can be used to create a point cloud representation of the environment.
In order to not interfere with other optical sensors (e.g. camera, human eye), LiDAR's wavelength is mainly located in the near-infrared part of the electromagnetic spectrum (750 nm to 1.5¬µm).
\\[6pt]
\textbf{Laser Doppler Vibrometer:} A laser microphone uses a laser beam to detect sound vibrations in a distant object. The minute differences in the distance traveled by the light as it reflects from the vibrating object are detected interferometrically. The Laser Doppler Vibrometer (LDV) implements this principle of laser interferometry by splitting the laser into two beams, one of which is reflected off the vibrating object. The surface will module the phase and frequency of the light due to the Doppler effect. One of the beams is passed through a Bragg cell (acousto-optic modulator) to add a frequency shift and then recombined with the other beam to be directed to a photodetector (Figure \ref{fig:LDV}). The electrical signal produced by the photodetector is equal to the carrier frequency produced by the Bragg cell modulated by the Doppler frequency of the vibrating object and proportional to the velocity of the object.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{embed/Lidar_scanner.jpg}
  \caption{Mechanical spinning LiDAR \cite{RenishawLidar}}
  \Description{Schematic of a mechanical spinning LiDAR}
  \label{fig:LiDAR}

  \vspace{0.5cm}

  \includegraphics[width=\linewidth]{embed/Laser_Doppler_Vibrometer.png}
  \caption{Laser Doppler Vibrometer \cite{WikipediaLDV}}
  \Description{Schematic of a Laser Doppler Vibrometer}
  \label{fig:LDV}
\end{figure}

\textbf{Recent Work:} In 2020, a group of researchers from the University of Singapore and the University of Maryland sparked the interest of the security research community \cite{BitdefenderRobotVacuumEavesdrop} and news outlets \cite{ForbesRobotVacuumEavesdrop} with their paper \textit{Spying with Your Robot Vacuum Cleaner: Eavesdropping via Lidar Sensors} ("Lidarphone") \cite{Lidarphone2020}. A method is introduced that repurposes lidar sensors in robot vacuum cleaners to function as laser-based microphones capable of capturing sound signals by detecting subtle vibrations on nearby objects.


\subsection{Other Eavesdropping Attacks}
\section{Threat Model}
\section{Speech Reconstruction}
\subsection{Speech Intelligibility}
A human speeking in a non-tonal language like English produces a complex waveform that is composed of various frequencies in the audible range.
While the fundamental frequency $f_0$ of the human voice is typically in the range of 100 Hz to 300 Hz (higher for women and children), overtones and consonant articulations can cover most of the audible frequency range of up to 17 kHz (Figure \ref{fig:SpeechFrequencySpectrum}).
Research has shown that frequencies between 1 kHz and 4 kHz are most important for speech intelligibility \cite{DPAMicrophonesFactsAboutSpeechIntelligibility}.
Applying a low-pass filter to the speech signal at 1 kHz and below quickly degrades the intelligibility of the speech to near zero as shown in Figure \ref{fig:SpeechIntelligibility}.
Since most experimets conducted using motion sensors are limited to a sampling rate of 100-500 Hz, special techniques have to be employed to recover frequencies above the Nyquist frequency $f_N = \frac{1}{2}f_s$ that are essential for speech intelligibility.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{embed/Speech_frequency_spectrum.jpg}
  \caption{Frequency spectrum of a human voice for Males, Females and Children \cite{DPAMicrophonesFactsAboutSpeechIntelligibility}}
  \Description{Frequency spectrum of a human voice}
  \label{fig:SpeechFrequencySpectrum}

  \vspace{0.5cm}

  \includegraphics[width=\linewidth]{embed/Speech_Intelligibility_Low_High_Pass_Filter.jpg}
  \caption{Speech intelligibility with low-pass and high-pass filters applied at various frequencies \cite{DPAMicrophonesFactsAboutSpeechIntelligibility}}
  \Description{Speech intelligibility with low-pass and high-pass filters}
  \label{fig:SpeechIntelligibility}
\end{figure}

\subsection{Signal Processing}
\subsection{Machine Learning}
\subsection{Automated Speech Recognition}
\section{Feasibility Study}
\section{Countermeasures}
\section{Conclusion}

\begin{table*}[t]
  \caption{Test parameters and key results from previous publications on vibration-based speech recovery attacks exploiting different sensors}
\label{tab:PapersParametersComparison}
\scriptsize
\renewcommand{\arraystretch}{1.3}
\begin{tabularx}{\textwidth}{@{} |lp{2.0cm}|CCCCCCCC|r| @{}}
\toprule
\textbf{Year} & \textbf{Paper} & \textbf{Sensor} & \textbf{Measuring Device} & \textbf{Attack Goal} & \textbf{Sample Freq.} & \textbf{Audio source} & \textbf{Transmission Medium} & \textbf{Distance from source} & \textbf{Dictionary Size} & \makecell{\textbf{Speech Rec.}\\\textbf{(best)}} \\
\hline
2014 & Gyrophone \cite{Gyrophone2014} & Gyroscope & Android Smartphone & Speech Rec., Speaker Ident., Gender Ident. & 200 Hz & External Loudspeaker & Solid Surface & 10 cm & 11 digits & 26 \% \\ \hline
2015 & AccelWorld \cite{AccelWorld2015} & Accelerometer & Android Smartphone & Speech Rec., Speaker Ident. & 200 Hz & External Loudspeaker & Air & 30 cm & 1 hotword & 85 \% \\ \hline
2017 & PitchIn \cite{PitchIn2017} & Accelerometer, Gyroscope, Geophone & Dedicated IMCU & Speech Rec. & 1 kHz & Human & Air & 1 m & 10 words & 79 \% \\ \hline
2018 & Speechless \cite{Speechless2018} & Accelerometer, Gyroscope & Android Smartphone & Speech Rec. & 200 Hz & External Loudspeaker & Solid Surface & 10 cm & 10 digits & 0 \% \\ \hline
2019 & Kinetic Song Comprehension \cite{KineticSongComprehension2019} & Accelerometer, Gyroscope & Android Smartphone & Song Rec. & 100 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 100 songs & 80 \% \\ \hline
2020 & AccelEve \cite{AccelEve2020} & Accelerometer & Android Smartphone & Speech Rec., Speaker Ident. & 100-500 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 8 hotwords & 90 \% \\ \hline
2021 & Spearphone \cite{Spearphone2021} & Accelerometer & Android Smartphone & Speech Rec., Speaker Ident., Gender Ident. & 120-500 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 58 words & 67 \% \\ \hline
2021 & Vibphone \cite{Vibphone2021} & Accelerometer & Android Smartphone & Speech Rec. & 225-425 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 10 hotwords + 10 digits & 54.2 \% \\ \hline
2022 & AccMyrinx \cite{AccMyrinx2022} & Accelerometer & Android Smartphone & Speech Rec. & 100-500 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & Synthesis & 57.33 \% \\ \hline
2022 & InertiEAR \cite{InertiEAR2022} & Accelerometer, Gyroscope & Smartphone & Speech Rec. & 40-200 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 10 digits & 49.8 \% \\ \hline
2023 & ISpyU \cite{ISpyU2023} & Accelerometer, Gyroscope & Android Smartphone & Automatic Speech Rec. & 200-500 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 9950 words & 53.3 \% \\ \hline
2023 & VoiceListener \cite{VoiceListener2023} & Accelerometer, Gyroscope, Magnetometer & Android Smartphone & Speech Rec. & 100-500 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 10 digits & 82.7 \% \\ \hline
2023 & StealthyIMU \cite{StealthyIMU2023} & Accelerometer & Android Smartphone & Speech Rec. (Voice Assistant) & 100-500 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 23 voice commands & - \\ \hline
2024 & Watch the Rhythm \cite{WatchTheRhythm2024} & Accelerometer & Android Smartphone & Speech Rec., Scene Rec. & 5-200 Hz & Smartphone Loudspeaker & Solid Surface & On-Device & 10 digits & 77.79 \% \\
\hline\hline
2020 & Lidarphone \cite{Lidarphone2020} & Lidar Scanner & Robot Vacuum Cleaner & Speech Rec., Song Rec., Speaker Ident., Gender Ident. & 1.8 kHz & External Loudspeaker & Air & 1.5 m & 10 digits & 91 \% \\
\hline\hline
2019 & Hard Drive of Hearing \cite{HardDriveOfHearing2019} & Hard Drive PES & HDD Controller & Speech Rec., Song Rec. & 34.56 kHz & External Loudspeaker & Air & 25 cm & - & 100 \% \\
\bottomrule
\end{tabularx}
\end{table*}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
